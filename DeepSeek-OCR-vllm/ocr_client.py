#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek OCR API Server (vLLM) - æ¥µç°¡ç‰ˆ
è¿”å› Markdown å…§å®¹ + åœ–åƒè³‡æ–™
"""
import os
import io
import re
import base64
import argparse
from io import BytesIO
from typing import List, Dict, Tuple

import torch
from PIL import Image, ImageDraw

try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

from vllm import LLM, SamplingParams
from vllm.model_executor.models.registry import ModelRegistry
from deepseek_ocr import DeepseekOCRForCausalLM
from process.ngram_norepeat import NoRepeatNGramLogitsProcessor
from process.image_process import DeepseekOCRProcessor

# -----------------------
# FastAPI App
# -----------------------
app = FastAPI(title="DeepSeek OCR API (vLLM) - Simple", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"], 
    allow_headers=["*"],
)

# -----------------------
# å…¨åŸŸè®Šæ•¸
# -----------------------
llm = None

# å›ºå®š Prompt
PROMPT_OCR = "<image>\n<|grounding|>Convert the document to markdown."
PROMPT_DESC = "<image>\nDescribe this image in detail."

# -----------------------
# æ¨¡çµ„ç´š Monkey Patch
# -----------------------
_original_tokenize = DeepseekOCRProcessor.tokenize_with_images

def _patched_tokenize(self, images, bos=True, eos=True, cropping=True, prompt=None):
    if prompt is not None:
        import config
        old = config.PROMPT
        config.PROMPT = prompt
        try:
            return _original_tokenize(self, images, bos, eos, cropping)
        finally:
            config.PROMPT = old
    return _original_tokenize(self, images, bos, eos, cropping)

DeepseekOCRProcessor.tokenize_with_images = _patched_tokenize

# -----------------------
# å·¥å…·å‡½å¼
# -----------------------
def pdf_to_images(pdf_bytes: bytes, dpi: int = 144) -> List[Image.Image]:
    """PDF è½‰åœ–ç‰‡"""
    if fitz is None:
        raise RuntimeError("æœªå®‰è£ PyMuPDFï¼Œè«‹åŸ·è¡Œ: pip install PyMuPDF")
    
    images = []
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    matrix = fitz.Matrix(dpi / 72.0, dpi / 72.0)
    
    for page in doc:
        pix = page.get_pixmap(matrix=matrix, alpha=False)
        img = Image.open(io.BytesIO(pix.tobytes("png")))
        
        if img.mode != "RGB":
            if img.mode in ("RGBA", "LA"):
                bg = Image.new("RGB", img.size, (255, 255, 255))
                bg.paste(img, mask=img.split()[-1])
                img = bg
            else:
                img = img.convert("RGB")
        
        images.append(img)
    
    doc.close()
    return images


def clear_vllm_cache():
    """æ¸…ç† vLLM å¿«å–"""
    if llm is None:
        return
    try:
        if hasattr(llm.llm_engine, 'input_preprocessor'):
            prep = llm.llm_engine.input_preprocessor
            if hasattr(prep, '_mm_processor_cache'):
                prep._mm_processor_cache.clear()
    except:
        pass


def vllm_generate(image: Image.Image, prompt: str) -> str:
    """vLLM æ¨ç†"""
    clear_vllm_cache()
    
    processor = DeepseekOCRProcessor()
    tokenized = processor.tokenize_with_images(images=[image], prompt=prompt)
    
    batch_inputs = [{
        "prompt": prompt,
        "multi_modal_data": {"image": tokenized}
    }]
    
    if prompt == PROMPT_OCR:
        logits_proc = [NoRepeatNGramLogitsProcessor(20, 50, {128821, 128822})]
        params = SamplingParams(
            temperature=0.0,
            max_tokens=4096,
            skip_special_tokens=False,
            logits_processors=logits_proc,
            repetition_penalty=1.05,
        )
    else:
        params = SamplingParams(
            temperature=0.0,
            max_tokens=512,
            skip_special_tokens=False,
        )
    
    outputs = llm.generate(batch_inputs, params)
    return outputs[0].outputs[0].text


def clean_markdown(text: str) -> str:
    """æ¸…ç† Markdown (ç§»é™¤ç‰¹æ®Šæ¨™è¨˜)"""
    # ç§»é™¤ <|ref|> <|det|> ç­‰æ¨™è¨˜
    text = re.sub(r'<\|ref\|>.*?<\|/ref\|>', '', text)
    text = re.sub(r'<\|det\|>.*?<\|/det\|>', '', text)
    text = re.sub(r'<\|.*?\|>', '', text)
    text = re.sub(r'\[\[.*?\]\]', '', text)
    
    # ç§»é™¤é•·åˆ†éš”ç·š
    text = re.sub(r'={50,}.*?={50,}', '', text, flags=re.DOTALL)
    
    # è§„èŒƒåŒ–ç©ºç™½
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def extract_images_from_raw(raw_text: str, source_image: Image.Image, page_idx: int) -> Tuple[str, Dict[str, str]]:
    """
    å¾ DeepSeek OCR åŸå§‹è¼¸å‡ºä¸­æå–åœ–åƒä¸¦è£åˆ‡

    åƒæ•¸:
        raw_text: OCR åŸå§‹è¼¸å‡ºï¼ˆåŒ…å« <|ref|>image<|/ref|><|det|>[[...]]<|/det|> æ¨™è¨˜ï¼‰
        source_image: åŸå§‹é é¢åœ–ç‰‡
        page_idx: é é¢ç´¢å¼•

    å›å‚³:
        (è™•ç†å¾Œçš„markdown, {image_name: base64_data})
    """
    images_dict = {}
    image_counter = 0

    # åœ–åƒæ¨™è¨˜æ¨¡å¼: <|ref|>image<|/ref|><|det|>[[x0,y0,x1,y1]]<|/det|>
    img_pattern = r'<\|ref\|>image<\|/ref\|><\|det\|>\[\[(.*?)\]\]<\|/det\|>'

    def replace_and_extract(match):
        nonlocal image_counter
        bbox_str = match.group(1)

        try:
            # è§£æ bbox: "x0,y0,x1,y1"
            coords = [float(x.strip()) for x in bbox_str.split(',')]
            if len(coords) >= 4:
                x0, y0, x1, y1 = coords[:4]

                # å–å¾—åœ–ç‰‡å°ºå¯¸
                img_width, img_height = source_image.size

                # è½‰æ›åº§æ¨™ï¼ˆDeepSeek åº§æ¨™æ˜¯æ­¸ä¸€åŒ–çš„ 0-1000ï¼‰
                x0_px = int(x0 * img_width / 1000)
                y0_px = int(y0 * img_height / 1000)
                x1_px = int(x1 * img_width / 1000)
                y1_px = int(y1 * img_height / 1000)

                # ç¢ºä¿åº§æ¨™åœ¨ç¯„åœå…§
                x0_px = max(0, min(x0_px, img_width))
                y0_px = max(0, min(y0_px, img_height))
                x1_px = max(0, min(x1_px, img_width))
                y1_px = max(0, min(y1_px, img_height))

                # è£åˆ‡åœ–åƒ
                if x1_px > x0_px and y1_px > y0_px:
                    cropped = source_image.crop((x0_px, y0_px, x1_px, y1_px))

                    # è½‰æ›ç‚º base64
                    buffered = BytesIO()
                    cropped.save(buffered, format="PNG")
                    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

                    # ç”¢ç”Ÿåœ–åƒåç¨±
                    image_name = f"page_{page_idx}_img_{image_counter}.png"
                    images_dict[image_name] = img_base64

                    image_counter += 1

                    # å›å‚³ markdown åœ–ç‰‡æ¨™è¨˜
                    return f"![Image {image_counter}]({image_name})"
        except Exception as e:
            print(f"âš ï¸  åœ–åƒæå–å¤±æ•—: {e}")

        return "[åœ–ç‰‡]"

    # æ›¿æ›æ‰€æœ‰åœ–åƒæ¨™è¨˜
    processed_text = re.sub(img_pattern, replace_and_extract, raw_text)

    return processed_text, images_dict


def generate_image_description(image: Image.Image) -> str:
    """ç”¢ç”Ÿåœ–ç‰‡æè¿°"""
    try:
        result = vllm_generate(image, PROMPT_DESC)

        # æ¸…ç†ç‰¹æ®Šæ¨™è¨˜
        desc = re.sub(r'<\|ref\|>.*?<\|/ref\|>', '', result)
        desc = re.sub(r'<\|det\|>.*?<\|/det\|>', '', desc)
        desc = re.sub(r'<\|.*?\|>', '', desc)
        desc = re.sub(r'\[\[.*?\]\]', '', desc)
        desc = re.sub(r'\s+', ' ', desc).strip()

        # æˆªæ–·åˆ°200å­—å…ƒ
        if len(desc) > 200:
            cutoff = desc[:200].rfind('.')
            if cutoff > 100:
                desc = desc[:cutoff + 1]
            else:
                desc = desc[:200].rsplit(' ', 1)[0] + '...'
        
        return desc
    except Exception as e:
        print(f"âš ï¸ åœ–ç‰‡æè¿°å¤±æ•—: {e}")
        return ""


# -----------------------
# æ¨¡å‹åˆå§‹åŒ–
# -----------------------
def initialize_model(model_path: str, gpu_id: int = 0):
    global llm
    
    ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)
    
    if torch.cuda.is_available():
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    os.environ['VLLM_USE_V1'] = '0'
    
    print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹: {model_path}")
    
    llm = LLM(
        model=model_path,
        hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
        block_size=256,
        enforce_eager=False,
        trust_remote_code=True,
        max_model_len=8192,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        max_num_seqs=100,
        disable_mm_preprocessor_cache=True,
    )
    
    print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")


# -----------------------
# API è·¯ç”±
# -----------------------
@app.get("/")
async def root():
    return {
        "service": "DeepSeek OCR (vLLM) - Simple",
        "version": "1.0.0",
        "status": "åŸ·è¡Œä¸­"
    }


@app.get("/health")
async def health():
    return {"status": "å¥åº·", "model_ready": llm is not None}


@app.post("/ocr")
async def ocr(
    file: UploadFile = File(...),
    enable_description: bool = Form(False),
):
    """
    OCR ä»‹é¢ (åœ–ç‰‡æˆ– PDF)
    
    åƒæ•¸:
        file: åœ–ç‰‡æª”æ¡ˆ (jpg/png) æˆ– PDF æª”æ¡ˆ
        enable_description: æ˜¯å¦ç”¢ç”Ÿåœ–ç‰‡æè¿°
    
    å›å‚³:
        {
            "markdown": "...",  # Markdown å…§å®¹
            "page_count": 1     # é æ•¸
        }
    """
    if llm is None:
        raise HTTPException(503, "æ¨¡å‹æœªè¼‰å…¥")
    
    try:
        contents = await file.read()
        
        # åˆ¤æ–·æª”æ¡ˆé¡å‹
        if file.filename.lower().endswith('.pdf'):
            images = pdf_to_images(contents)
        else:
            images = [Image.open(BytesIO(contents)).convert("RGB")]
        
        print(f"ğŸ“„ è™•ç† {len(images)} é ...")
        
        # è™•ç†æ¯ä¸€é 
        md_parts = []
        all_images = {}  # å„²å­˜æ‰€æœ‰æå–çš„åœ–åƒ {image_name: base64_data}

        for idx, img in enumerate(images):
            print(f"   é  {idx + 1}/{len(images)}")

            # OCR
            raw = vllm_generate(img, PROMPT_OCR)

            # æå–åœ–åƒï¼ˆåœ¨æ¸…ç†ç‰¹æ®Šæ¨™è¨˜ä¹‹å‰ï¼‰
            processed_text, page_images = extract_images_from_raw(raw, img, idx)
            all_images.update(page_images)

            print(f"      æå–äº† {len(page_images)} å¼µåœ–ç‰‡")

            # å¦‚æœå•Ÿç”¨åœ–ç‰‡æè¿°,æ›¿æ›åœ–ç‰‡æ¨™è¨˜
            if enable_description:
                # å°‹æ‰¾æ‰€æœ‰ <|ref|>image<|/ref|> æ¨™è¨˜
                img_pattern = r'<\|ref\|>image<\|/ref\|><\|det\|>\[\[.*?\]\]<\|/det\|>'

                def replace_with_desc(match):
                    # æå– bbox
                    det_match = re.search(r'\[\[(.*?)\]\]', match.group(0))
                    if det_match:
                        # ç”¢ç”Ÿæè¿°
                        desc = generate_image_description(img)
                        return f"[åœ–ç‰‡: {desc}]" if desc else "[åœ–ç‰‡]"
                    return "[åœ–ç‰‡]"

                processed_text = re.sub(img_pattern, replace_with_desc, processed_text)

            # æ¸…ç†ä¸¦åŠ å…¥
            cleaned = clean_markdown(processed_text)
            if cleaned:
                md_parts.append(cleaned)

        # åˆä½µæ‰€æœ‰é 
        final_md = "\n\n---\n\n".join(md_parts)

        print(f"âœ… å®Œæˆï¼ç¸½å…±æå– {len(all_images)} å¼µåœ–ç‰‡")

        # å›å‚³çµæœï¼ˆåŒ…å«åœ–åƒè³‡æ–™ï¼‰
        response_data = {
            "markdown": final_md,
            "page_count": len(images)
        }

        # å¦‚æœæœ‰åœ–åƒï¼ŒåŠ å…¥å›æ‡‰ä¸­
        if all_images:
            response_data["images"] = all_images

        return JSONResponse(response_data)
    
    except Exception as e:
        import traceback
        raise HTTPException(500, f"è™•ç†å¤±æ•—: {e}\n{traceback.format_exc()}")


# -----------------------
# å•Ÿå‹•
# -----------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", required=True, help="æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--gpu-id", type=int, default=0, help="GPU ID")
    parser.add_argument("--port", type=int, default=8707, help="åŸ è™Ÿ")
    parser.add_argument("--host", default="0.0.0.0", help="ç›£è½ä½å€")
    
    args = parser.parse_args()
    
    initialize_model(args.model_path, args.gpu_id)
    
    print(f"\nğŸš€ æœå‹™å•Ÿå‹•: http://{args.host}:{args.port}")
    print(f"ğŸ“– ä»‹é¢æ–‡ä»¶: http://{args.host}:{args.port}/docs\n")
    
    uvicorn.run(app, host=args.host, port=args.port, workers=1)


if __name__ == "__main__":
    main()
